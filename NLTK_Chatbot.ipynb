{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Описание**\n",
        "Бот использует нейронные сети, чтобы научиться распознавать паттерны в сообщениях пользователя, и отвечать соответствующим образом.\n",
        "Бот анализирует сообщения на английском языке.\n",
        "\n",
        "\n",
        "Ответы прописаны заранее для описанных классов сообщений. В качестве ответа вы получаете одно из подготовленных сообщений на соответствующую тематику.\n",
        "\n",
        "---\n",
        "Классы создаются вручную в файле json.\n",
        "В загружаемом файле описаны классы:\n",
        "* Hello - приветствия\n",
        "* Bot - вопросы о боте\n",
        "* Age - вопросы о возрасте\n",
        "* Annoying - сообщения по типу \"ты меня достал\"\n",
        "* Question - просьба ответить на вопрос\n",
        "* Help - просьба о помощи\n",
        "* Exit - прощание\n",
        "* SupvervLearn - машинное обучение\n",
        "* NN - нейронные сети\n",
        "* Bully - оскорбления\n",
        "* Ticket - оценка ответа\n",
        "\n",
        "Внутри каждого класса задается набор сообщений, которые выступают в качестве паттернов. Из этих паттернов создается обучающая выборка.\n",
        "\n",
        "Модель учится распознавать паттерны в полученном сообщении, после чего она относит сообщение к заданному классу и отвечает одним из вариантов заранее заданных ответов для класса.\n",
        "\n",
        "\n",
        "---\n",
        "При желании можно дополнить json файл своими классами или дописать больше примеров паттернов в уже существующие, это должно повысить аккуратность модели.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "> # <font color='red'> Перед использованием бота необходимо целиком запустить весь ноутбук </font>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nc1mn6Kq5D7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Модель**"
      ],
      "metadata": {
        "id": "8jh7SyuIB5o7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подключаем Google Drive, чтобы скачать обучающий датасет."
      ],
      "metadata": {
        "id": "9k7a9_pxuL9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "\n",
        "import gdown\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1p7-hWXUiMLftwjFHpVK0GXqaikT5A6md'\n",
        "output = 'Train_Bot.json'\n",
        "\n",
        "gdown.download(url, output, quiet=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "QvTccxCRXsW0",
        "outputId": "02e4fa17-ba54-4ead-ef6e-36c8b22c7228"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.11.17)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1p7-hWXUiMLftwjFHpVK0GXqaikT5A6md\n",
            "To: /content/Train_Bot.json\n",
            "100%|██████████| 6.07k/6.07k [00:00<00:00, 5.38MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Train_Bot.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vixmopIVshk8"
      },
      "source": [
        "---\n",
        "### Подключаем библиотеки\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wj8edLPwshk9"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import random\n",
        "from keras.models import load_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MItt3kYshk-"
      },
      "source": [
        "---\n",
        "### Предобработка данных\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Iw-nNla4shk-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94a3d230-86f3-4d1a-a820-e9547fec1185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "words=[]\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?', '!', '.', ',']\n",
        "data_file = open(\"Train_Bot.json\").read()\n",
        "intents = json.loads(data_file)\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "\n",
        "        #токенизируем предложения из pattern\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        words.extend(w)\n",
        "\n",
        "        #добавляем предложения в категорию 'tag'\n",
        "        documents.append((w, intent['tag']))\n",
        "\n",
        "        # добавляем тег в класс\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BMx_zKyshk_",
        "outputId": "71664701-6b6a-4f51-ef06-860ed09e7ecc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "131 documents\n",
            "11 classes ['Age', 'Annoying', 'Bot', 'Bully', 'Exit', 'Hello', 'Help', 'NN', 'Question', 'SupvervLearn', 'Ticket']\n",
            "155 unique lemmatized words [\"'d\", \"'re\", 'a', 'able', 'about', 'activation', 'ada', 'adam', 'afternoon', 'age', 'am', 'an', 'ann', 'annoy', 'annoying', 'answer', 'are', 'artificial', 'assistance', 'backward', 'bad', 'bagging', 'bayes', 'best', 'better', 'bloody', 'boosting', 'bot', 'buddy', 'can', 'classification', 'create', 'cross', 'cya', 'day', 'deep', 'define', 'did', 'diffult', 'do', 'ensemble', 'epoch', 'explain', 'forest', 'forward', 'function', 'give', 'good', 'goodbye', 'gradient', 'great', 'greet', 'hate', 'have', 'hell', 'hello', 'help', 'helped', 'hey', 'hi', 'hidden', 'how', 'howdy', 'hyper', 'i', 'imputer', 'incredibly', 'intelligence', 'irritating', 'is', 'jerk', 'joke', 'just', 'knn', 'know', 'later', 'layer', 'learning', 'leaving', 'like', 'logistic', 'lot', 'machine', 'me', 'ml', 'much', 'my', 'naive', 'nb', 'need', 'net', 'network', 'neural', 'no', 'not', 'now', 'of', 'old', 'otimizer', 'parameter', 'personality', 'piece', 'problem', 'propagation', 'question', 'random', 'regression', 'relu', 'right', 'say', 'screw', 'see', 'sgd', 'shit', 'sigmoid', 'sl', 'smart', 'so', 'softmax', 'solution', 'solved', 'some', 'stuff', 'stupid', 'such', 'supervised', 'svm', 'talk', 'techb=niques', 'technique', 'tell', 'thank', 'thanks', 'the', 'there', 'think', 'ticket', 'time', 'to', 'ton', 'too', 'unable', 'understand', 'useless', 'validation', 'very', 'want', 'wasted', 'weight', 'what', 'work', 'you', 'your', 'yours', 'yourself']\n"
          ]
        }
      ],
      "source": [
        "# получаем список уникальных лемматизированных слов из pattern\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "# выделяем уникальные классы\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "print (len(documents), \"documents\")\n",
        "print (len(classes), \"classes\", classes)\n",
        "print (len(words), \"unique lemmatized words\", words)\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "# сохраняем полученные данные\n",
        "pickle.dump(words,open('words.pkl','wb'))\n",
        "pickle.dump(classes,open('classes.pkl','wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR-oqJ0wshlA"
      },
      "source": [
        "---\n",
        "### Создаем обучающую выборку\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoM-44d9shlA",
        "outputId": "ac15f1a6-868f-4d86-a3dd-af033a1fb2d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-c7fe597d34e8>:28: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  training = np.array(training)\n"
          ]
        }
      ],
      "source": [
        "training = []\n",
        "\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "for doc in documents:\n",
        "\n",
        "    bag = []\n",
        "\n",
        "    # список слов из pattern\n",
        "    pattern_words = doc[0]\n",
        "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "\n",
        "    # добавляем слово в мешок слов, если совпадение нашлось в текущем pattern\n",
        "    for w in words:\n",
        "      if w in pattern_words:\n",
        "        bag.append(1)\n",
        "      else:\n",
        "        bag.append(0)\n",
        "\n",
        "    # помечаем текущий tag 1, остальные 0\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "\n",
        "\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "X_train = list(training[:,0])\n",
        "y_train = list(training[:,1])\n",
        "\n",
        "print(\"Training data created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJM2R1NHshlB"
      },
      "source": [
        "---\n",
        "### Создаем нейронную сеть\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VWroJgFshlB",
        "outputId": "288e7423-9aec-4d35-9c60-fe891b7b4e9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(X_train[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "# softmax чтобы в сумме получали 1, т.е. имеем вероятности\n",
        "model.add(Dense(len(y_train[0]), activation='softmax'))\n",
        "\n",
        "# Стохастический градиентный спуск\n",
        "sgd = SGD(learning_rate=0.01, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "NN = model.fit(np.array(X_train), np.array(y_train), epochs=200, batch_size=5, verbose=False)\n",
        "\n",
        "model.save('chatbot.h5', NN)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "JZx_lgMjshlC"
      },
      "source": [
        "---\n",
        "## Модель обучена. Приступим к использованию.\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xZMwR2FUshlC"
      },
      "outputs": [],
      "source": [
        "# загружаем модель\n",
        "model = load_model('chatbot.h5')\n",
        "intents = json.loads(open(\"Train_Bot.json\").read())\n",
        "words = pickle.load(open('words.pkl','rb'))\n",
        "classes = pickle.load(open('classes.pkl','rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "C8ctzYIUshlC"
      },
      "outputs": [],
      "source": [
        "def clean_up_sentence(sentence):\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "\n",
        "\n",
        "# возвращаем массив содержащий 0 или 1, показывающее, встретилось ли слово в предложении\n",
        "def bow(sentence, words):\n",
        "\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "\n",
        "    bag = [0] * len(words)\n",
        "\n",
        "    for s in sentence_words:\n",
        "        for i, w in enumerate(words):\n",
        "            if w == s:\n",
        "                bag[i] = 1\n",
        "    return(np.array(bag))\n",
        "\n",
        "\n",
        "def predict_class(sentence, model):\n",
        "    b = bow(sentence, words)\n",
        "    result = model.predict(np.array([b]))[0]\n",
        "    # threshold для обрезки незначимых значений\n",
        "    error = 0.25\n",
        "    results = [[i, res] for i, res in enumerate(result) if res > error]\n",
        "\n",
        "    # сортируем по убыванию вероятностей\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "    return_list = []\n",
        "\n",
        "    for res in results:\n",
        "        return_list.append({\"intent\": classes[res[0]], \"probability\": str(res[1])})\n",
        "    return return_list\n",
        "\n",
        "\n",
        "\n",
        "def getResponse(ints, intents_json):\n",
        "    tag = ints[0]['intent']\n",
        "    list_of_intents = intents_json['intents']\n",
        "    for i in list_of_intents:\n",
        "        if(i['tag']== tag):\n",
        "            result = random.choice(i['responses'])\n",
        "            break\n",
        "    return result\n",
        "\n",
        "\n",
        "def chatbot_response(text):\n",
        "    ints = predict_class(text, model)\n",
        "    res = getResponse(ints, intents)\n",
        "    return res\n",
        "\n",
        "\n",
        "\n",
        "# начинаем общение с ботом пока не напишем 'end'\n",
        "\n",
        "def start_chat():\n",
        "    print(\"Bot: This is ANDY! Your Personal Assistant.\\n\\n\")\n",
        "    while True:\n",
        "        inp = str(input()).lower()\n",
        "        if inp.lower()==\"end\":\n",
        "            break\n",
        "        if inp.lower()== '' or inp.lower()== '*':\n",
        "            print('Please re-phrase your query!')\n",
        "            print(\"-\"*50)\n",
        "        else:\n",
        "            print(f\"Bot: {chatbot_response(inp)}\"+'\\n')\n",
        "            print(\"-\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obm5NUt9shlD"
      },
      "source": [
        "## <font color = 'green'> Пообщаемся с нашим ботом! </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4U0yon_pshlD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afe4aa9c-a2bf-440d-9ac6-997b60fa4b94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: This is ANDY! Your Personal Assistant.\n",
            "\n",
            "\n",
            "Hello!\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Bot: How are you?\n",
            "\n",
            "--------------------------------------------------\n",
            "Tell me about yourself\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Bot: I am an artificial intelligence.\n",
            "\n",
            "--------------------------------------------------\n",
            "How old are you?\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Bot: I have no age, I'm only getting newer every day.\n",
            "\n",
            "--------------------------------------------------\n",
            "Okay. Can you help me>\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Bot: I'm glad to help. What can I do for you?\n",
            "\n",
            "--------------------------------------------------\n",
            "Do you like neural networks?\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Bot: Neural Networks are cool! Part of my intelligence is NN too! Check https://en.wikipedia.org/wiki/Neural_network for more information \n",
            "\n",
            "--------------------------------------------------\n",
            "NIce! Thanks, bye!\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Bot: I hope I was able to assist you, Good Bye\n",
            "\n",
            "--------------------------------------------------\n",
            "end\n"
          ]
        }
      ],
      "source": [
        "start_chat()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Источники\n",
        "Идею взял отсюда: https://www.pycodemates.com/2021/11/build-a-AI-chatbot-using-python-and-deep-learning.html"
      ],
      "metadata": {
        "id": "GGqY3g0RUUjv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "При проверке работы бота были обнаружены некоторые \"аномалии\":\n",
        "* Сообщения, содержащие 'are' скорее всего, будут ложно классифицированы как сообщения класса Annoying, так как 'are' чаще всего встречается в обучающей выборке именно в этом классе."
      ],
      "metadata": {
        "id": "VbO5llIGAs3P"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}